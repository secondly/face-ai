#!/usr/bin/env python3
"""
ç®€åŒ–çš„CUDAæ£€æŸ¥å·¥å…·
é¿å…å¤æ‚æ“ä½œå¯¼è‡´ç•Œé¢å¡æ­»
"""

import subprocess
import sys
from typing import Dict

def quick_cuda_check() -> Dict:
    """å¿«é€ŸCUDAæ£€æŸ¥"""
    result = {
        'onnxruntime_info': check_onnxruntime(),
        'cuda_available': check_cuda_command(),
        'gpu_available': check_nvidia_smi(),
        'main_issue': None,
        'simple_solution': None,
        'status': 'unknown'
    }
    
    # åˆ†æä¸»è¦é—®é¢˜
    result['main_issue'], result['simple_solution'] = analyze_main_issue(result)
    
    # ç¡®å®šçŠ¶æ€
    if result['main_issue'] is None:
        result['status'] = 'good'
    elif 'onnxruntime' in result['main_issue'].lower():
        result['status'] = 'fixable'
    else:
        result['status'] = 'warning'
    
    return result

def check_onnxruntime() -> Dict:
    """æ£€æŸ¥ONNX Runtime"""
    try:
        import onnxruntime as ort
        version = ort.__version__
        providers = ort.get_available_providers()
        
        # æ£€æŸ¥å®‰è£…çš„åŒ…
        try:
            import pkg_resources
            installed_packages = [pkg.project_name.lower() for pkg in pkg_resources.working_set]
            has_gpu_package = 'onnxruntime-gpu' in installed_packages
            has_cpu_package = 'onnxruntime' in installed_packages
        except:
            has_gpu_package = False
            has_cpu_package = True
        
        return {
            'installed': True,
            'version': version,
            'providers': providers,
            'has_cuda_provider': 'CUDAExecutionProvider' in providers,
            'has_gpu_package': has_gpu_package,
            'has_cpu_package': has_cpu_package
        }
    except ImportError:
        return {
            'installed': False
        }

def check_cuda_command() -> bool:
    """æ£€æŸ¥CUDAå‘½ä»¤"""
    try:
        result = subprocess.run(['nvcc', '--version'], 
                              capture_output=True, text=True, timeout=5)
        return result.returncode == 0
    except:
        return False

def check_nvidia_smi() -> bool:
    """æ£€æŸ¥nvidia-smi"""
    try:
        result = subprocess.run(['nvidia-smi'], 
                              capture_output=True, text=True, timeout=5)
        return result.returncode == 0
    except:
        return False

def analyze_main_issue(result: Dict) -> tuple:
    """åˆ†æä¸»è¦é—®é¢˜"""
    onnx_info = result['onnxruntime_info']
    
    if not onnx_info.get('installed'):
        return "ONNX Runtimeæœªå®‰è£…", "å®‰è£…ONNX Runtime GPUç‰ˆæœ¬"
    
    if not result['gpu_available']:
        return "æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–é©±åŠ¨é—®é¢˜", "æ£€æŸ¥GPUé©±åŠ¨ç¨‹åºå®‰è£…"
    
    if not result['cuda_available']:
        return "CUDAæœªå®‰è£…", "å®‰è£…CUDA Toolkit"
    
    if not onnx_info.get('has_cuda_provider'):
        return "ONNX Runtimeä¸æ”¯æŒCUDA", "å®‰è£…onnxruntime-gpuç‰ˆæœ¬"
    
    if onnx_info.get('has_cpu_package') and not onnx_info.get('has_gpu_package'):
        return "å®‰è£…äº†CPUç‰ˆæœ¬çš„ONNX Runtime", "å¸è½½onnxruntimeï¼Œå®‰è£…onnxruntime-gpu"
    
    if onnx_info.get('has_cpu_package') and onnx_info.get('has_gpu_package'):
        return "åŒæ—¶å®‰è£…äº†CPUå’ŒGPUç‰ˆæœ¬", "å¸è½½æ‰€æœ‰ç‰ˆæœ¬ï¼Œé‡æ–°å®‰è£…onnxruntime-gpu"
    
    # ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
    version = onnx_info.get('version', '')
    if version.startswith('1.19') or version.startswith('1.18'):
        return "ONNX Runtimeç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹CUDA 12.3", "å®‰è£…å…¼å®¹ç‰ˆæœ¬ï¼šonnxruntime-gpu==1.16.3"
    
    return None, None

def test_gpu_simple() -> Dict:
    """ç®€å•çš„GPUæµ‹è¯•"""
    try:
        import onnxruntime as ort
        
        # åªæ£€æŸ¥æä¾›è€…ï¼Œä¸åˆ›å»ºä¼šè¯
        providers = ort.get_available_providers()
        
        if 'CUDAExecutionProvider' in providers:
            return {
                'test_result': 'cuda_available',
                'message': 'CUDAæä¾›è€…å¯ç”¨'
            }
        elif 'DmlExecutionProvider' in providers:
            return {
                'test_result': 'directml_available', 
                'message': 'DirectMLæä¾›è€…å¯ç”¨'
            }
        else:
            return {
                'test_result': 'cpu_only',
                'message': 'ä»…CPUæä¾›è€…å¯ç”¨'
            }
    except ImportError:
        return {
            'test_result': 'no_onnxruntime',
            'message': 'ONNX Runtimeæœªå®‰è£…'
        }
    except Exception as e:
        return {
            'test_result': 'error',
            'message': f'æµ‹è¯•å¤±è´¥: {str(e)}'
        }

def format_simple_report(result: Dict) -> str:
    """æ ¼å¼åŒ–ç®€å•æŠ¥å‘Š"""
    report = "ğŸ” å¿«é€ŸGPUé…ç½®æ£€æŸ¥\n"
    report += "=" * 40 + "\n\n"
    
    # ONNX RuntimeçŠ¶æ€
    onnx_info = result['onnxruntime_info']
    if onnx_info.get('installed'):
        report += f"ğŸ“¦ ONNX Runtime: âœ… å·²å®‰è£… (v{onnx_info.get('version', 'Unknown')})\n"
        report += f"   GPUåŒ…: {'âœ…' if onnx_info.get('has_gpu_package') else 'âŒ'}\n"
        report += f"   CUDAæ”¯æŒ: {'âœ…' if onnx_info.get('has_cuda_provider') else 'âŒ'}\n"
    else:
        report += "ğŸ“¦ ONNX Runtime: âŒ æœªå®‰è£…\n"
    
    # GPUçŠ¶æ€
    report += f"\nğŸ® NVIDIA GPU: {'âœ…' if result['gpu_available'] else 'âŒ'}\n"
    report += f"ğŸš€ CUDA: {'âœ…' if result['cuda_available'] else 'âŒ'}\n"
    
    # ä¸»è¦é—®é¢˜
    if result['main_issue']:
        report += f"\nâŒ ä¸»è¦é—®é¢˜: {result['main_issue']}\n"
        report += f"ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ: {result['simple_solution']}\n"
    else:
        report += f"\nâœ… é…ç½®çœ‹èµ·æ¥æ­£å¸¸\n"
    
    # GPUæµ‹è¯•
    gpu_test = test_gpu_simple()
    report += f"\nğŸ§ª GPUæµ‹è¯•: {gpu_test['message']}\n"
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    result = quick_cuda_check()
    report = format_simple_report(result)
    print(report)
    return result

if __name__ == "__main__":
    main()
