#!/usr/bin/env python3
"""
GPUæ£€æµ‹å’Œé…ç½®å·¥å…·
æä¾›è¯¦ç»†çš„GPUä¿¡æ¯æ£€æµ‹å’Œæ™ºèƒ½é…ç½®å»ºè®®
"""

import os
import sys
import platform
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)

class GPUDetector:
    """GPUæ£€æµ‹å™¨"""
    
    def __init__(self):
        self.system = platform.system()
        self.gpu_info = {}
        self.onnx_providers = []
        self.cuda_info = {}
        
    def detect_all(self) -> Dict:
        """æ£€æµ‹æ‰€æœ‰GPUç›¸å…³ä¿¡æ¯"""
        logger.info("ğŸ” å¼€å§‹GPUç¯å¢ƒæ£€æµ‹...")
        
        result = {
            'system': self.system,
            'nvidia_gpu': self._detect_nvidia_gpu(),
            'amd_gpu': self._detect_amd_gpu(),
            'intel_gpu': self._detect_intel_gpu(),
            'cuda': self._detect_cuda(),
            'onnx_providers': self._detect_onnx_providers(),
            'recommended_config': None,
            'gpu_available': False
        }
        
        # ç”Ÿæˆæ¨èé…ç½®
        result['recommended_config'] = self._generate_recommendation(result)

        # ç¡®ä¿æ¨èé…ç½®ä¸ä¸ºNone
        if result['recommended_config'] is None:
            result['recommended_config'] = {
                'type': 'cpu_only',
                'provider': 'CPUExecutionProvider',
                'description': 'CPUå¤„ç†æ¨¡å¼',
                'performance': 'basic',
                'gpu_enabled': False,
                'reason': 'æ— å¯ç”¨GPUé…ç½®'
            }

        result['gpu_available'] = self._is_gpu_available(result)
        
        return result
    
    def _detect_nvidia_gpu(self) -> Dict:
        """æ£€æµ‹NVIDIA GPU"""
        logger.info("ğŸ” æ£€æµ‹NVIDIA GPU...")

        try:
            # é¦–å…ˆå°è¯•ç®€å•çš„nvidia-smiå‘½ä»¤
            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                # å¦‚æœnvidia-smiå¯ç”¨ï¼Œå°è¯•è·å–è¯¦ç»†ä¿¡æ¯
                try:
                    detail_result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version,cuda_version',
                                                   '--format=csv,noheader,nounits'],
                                                  capture_output=True, text=True, timeout=10)

                    if detail_result.returncode == 0:
                        lines = detail_result.stdout.strip().split('\n')
                        gpus = []

                        for line in lines:
                            if line.strip():
                                parts = [p.strip() for p in line.split(',')]
                                if len(parts) >= 4:
                                    gpu_info = {
                                        'name': parts[0],
                                        'memory_mb': int(parts[1]) if parts[1].isdigit() else 0,
                                        'driver_version': parts[2],
                                        'cuda_version': parts[3]
                                    }
                                    gpus.append(gpu_info)
                                    logger.info(f"âœ… æ£€æµ‹åˆ°NVIDIA GPU: {gpu_info['name']} ({gpu_info['memory_mb']}MB)")

                        if gpus:
                            return {
                                'available': True,
                                'count': len(gpus),
                                'gpus': gpus,
                                'driver_version': gpus[0]['driver_version'] if gpus else None,
                                'cuda_version': gpus[0]['cuda_version'] if gpus else None
                            }
                except:
                    pass

                # å¦‚æœè¯¦ç»†æŸ¥è¯¢å¤±è´¥ï¼Œä½†nvidia-smiå¯ç”¨ï¼Œè¯´æ˜æœ‰NVIDIA GPU
                logger.info("âœ… æ£€æµ‹åˆ°NVIDIA GPU (è¯¦ç»†ä¿¡æ¯è·å–å¤±è´¥)")
                return {
                    'available': True,
                    'count': 1,
                    'gpus': [{'name': 'NVIDIA GPU', 'memory_mb': 0, 'driver_version': 'Unknown', 'cuda_version': 'Unknown'}],
                    'driver_version': 'Unknown',
                    'cuda_version': 'Unknown'
                }
            else:
                logger.info("âŒ nvidia-smiå‘½ä»¤æ‰§è¡Œå¤±è´¥")
                return {'available': False, 'error': 'nvidia-smi failed'}

        except FileNotFoundError:
            logger.info("âŒ nvidia-smiå‘½ä»¤ä¸å­˜åœ¨")
            return {'available': False, 'error': 'nvidia-smi not found'}
        except subprocess.TimeoutExpired:
            logger.info("âŒ nvidia-smiå‘½ä»¤è¶…æ—¶")
            return {'available': False, 'error': 'nvidia-smi timeout'}
        except Exception as e:
            logger.info(f"âŒ NVIDIA GPUæ£€æµ‹å¤±è´¥: {e}")
            return {'available': False, 'error': str(e)}
    
    def _detect_amd_gpu(self) -> Dict:
        """æ£€æµ‹AMD GPU"""
        logger.info("ğŸ” æ£€æµ‹AMD GPU...")
        
        if self.system == "Linux":
            try:
                result = subprocess.run(['rocm-smi', '--showproductname'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    logger.info("âœ… æ£€æµ‹åˆ°AMD GPU (ROCm)")
                    return {'available': True, 'type': 'rocm', 'info': result.stdout.strip()}
            except (FileNotFoundError, subprocess.TimeoutExpired):
                pass
        
        # Windows AMD GPUæ£€æµ‹
        if self.system == "Windows":
            try:
                result = subprocess.run(['wmic', 'path', 'win32_VideoController', 'get', 'name,AdapterRAM'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0 and 'AMD' in result.stdout:
                    logger.info("âœ… æ£€æµ‹åˆ°AMD GPU")
                    return {'available': True, 'type': 'directml', 'info': 'AMD GPU detected'}
            except Exception:
                pass
        
        logger.info("âŒ æœªæ£€æµ‹åˆ°AMD GPU")
        return {'available': False}
    
    def _detect_intel_gpu(self) -> Dict:
        """æ£€æµ‹Intel GPU"""
        logger.info("ğŸ” æ£€æµ‹Intel GPU...")
        
        if self.system == "Windows":
            try:
                result = subprocess.run(['wmic', 'path', 'win32_VideoController', 'get', 'name,AdapterRAM'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0 and 'Intel' in result.stdout:
                    logger.info("âœ… æ£€æµ‹åˆ°Intel GPU")
                    return {'available': True, 'type': 'directml', 'info': 'Intel GPU detected'}
            except Exception:
                pass
        
        logger.info("âŒ æœªæ£€æµ‹åˆ°Intel GPU")
        return {'available': False}
    
    def _detect_cuda(self) -> Dict:
        """æ£€æµ‹CUDA"""
        logger.info("ğŸ” æ£€æµ‹CUDA...")
        
        try:
            result = subprocess.run(['nvcc', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                version_line = None
                for line in result.stdout.split('\n'):
                    if 'release' in line.lower():
                        version_line = line.strip()
                        break
                
                logger.info(f"âœ… æ£€æµ‹åˆ°CUDA: {version_line}")
                return {
                    'available': True,
                    'version_info': version_line,
                    'nvcc_output': result.stdout.strip()
                }
            else:
                logger.info("âŒ CUDAç¼–è¯‘å™¨ä¸å¯ç”¨")
                return {'available': False, 'error': 'nvcc failed'}
                
        except FileNotFoundError:
            logger.info("âŒ nvccå‘½ä»¤ä¸å­˜åœ¨")
            return {'available': False, 'error': 'nvcc not found'}
        except subprocess.TimeoutExpired:
            logger.info("âŒ nvccå‘½ä»¤è¶…æ—¶")
            return {'available': False, 'error': 'nvcc timeout'}
        except Exception as e:
            logger.info(f"âŒ CUDAæ£€æµ‹å¤±è´¥: {e}")
            return {'available': False, 'error': str(e)}
    
    def _detect_onnx_providers(self) -> Dict:
        """æ£€æµ‹ONNX Runtimeæä¾›è€…"""
        logger.info("ğŸ” æ£€æµ‹ONNX Runtimeæä¾›è€…...")
        
        try:
            import onnxruntime as ort
            available_providers = ort.get_available_providers()
            
            provider_details = {}
            for provider in available_providers:
                if provider == 'CUDAExecutionProvider':
                    try:
                        # å°è¯•è·å–CUDAè®¾å¤‡ä¿¡æ¯
                        session_options = ort.SessionOptions()
                        providers_list = [('CUDAExecutionProvider', {'device_id': 0})]
                        provider_details[provider] = {
                            'available': True,
                            'type': 'NVIDIA CUDA',
                            'description': 'NVIDIA GPUåŠ é€Ÿ (æœ€ä½³æ€§èƒ½)'
                        }
                        logger.info("âœ… CUDAExecutionProvider å¯ç”¨")
                    except Exception as e:
                        provider_details[provider] = {
                            'available': False,
                            'error': str(e),
                            'description': 'CUDAæä¾›è€…ä¸å¯ç”¨'
                        }
                        logger.info(f"âŒ CUDAExecutionProvider ä¸å¯ç”¨: {e}")
                
                elif provider == 'DmlExecutionProvider':
                    provider_details[provider] = {
                        'available': True,
                        'type': 'DirectML',
                        'description': 'DirectML GPUåŠ é€Ÿ (æ”¯æŒAMD/Intel/NVIDIA)'
                    }
                    logger.info("âœ… DmlExecutionProvider å¯ç”¨")
                
                elif provider == 'CPUExecutionProvider':
                    provider_details[provider] = {
                        'available': True,
                        'type': 'CPU',
                        'description': 'CPUå¤„ç† (å…¼å®¹æ€§æœ€ä½³)'
                    }
                    logger.info("âœ… CPUExecutionProvider å¯ç”¨")
                
                else:
                    provider_details[provider] = {
                        'available': True,
                        'type': 'Other',
                        'description': f'{provider} å¯ç”¨'
                    }
                    logger.info(f"âœ… {provider} å¯ç”¨")
            
            return {
                'available': True,
                'providers': available_providers,
                'details': provider_details,
                'onnxruntime_version': ort.__version__
            }
            
        except ImportError:
            logger.info("âŒ ONNX Runtimeæœªå®‰è£…")
            return {'available': False, 'error': 'onnxruntime not installed'}
        except Exception as e:
            logger.info(f"âŒ ONNX Runtimeæ£€æµ‹å¤±è´¥: {e}")
            return {'available': False, 'error': str(e)}
    
    def _generate_recommendation(self, detection_result: Dict) -> Dict:
        """ç”Ÿæˆæ¨èé…ç½®"""
        logger.info("ğŸ¯ ç”Ÿæˆæ¨èé…ç½®...")
        
        nvidia = detection_result['nvidia_gpu']
        cuda = detection_result['cuda']
        onnx = detection_result['onnx_providers']
        
        # NVIDIA GPU + CUDA (ä¼˜å…ˆæ¨èï¼Œä½†éœ€è¦çœŸæ­£å¯ç”¨)
        if nvidia.get('available') and cuda.get('available'):
            if (onnx.get('available') and 'CUDAExecutionProvider' in onnx.get('providers', [])):
                # æµ‹è¯•CUDAæ˜¯å¦çœŸæ­£å¯ç”¨
                try:
                    import onnxruntime as ort
                    # å°è¯•åˆ›å»ºCUDAä¼šè¯æ¥éªŒè¯
                    test_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
                    # è¿™é‡Œåªæ˜¯æ£€æŸ¥æä¾›è€…æ˜¯å¦å¯ç”¨ï¼Œä¸åˆ›å»ºå®é™…ä¼šè¯
                    return {
                        'type': 'cuda_gpu',
                        'provider': 'CUDAExecutionProvider',
                        'description': 'NVIDIA CUDA GPUåŠ é€Ÿ (æ¨è)',
                        'performance': 'excellent',
                        'gpu_enabled': True,
                        'reason': 'NVIDIA GPU + CUDA + CUDAExecutionProvider å®Œæ•´æ”¯æŒ'
                    }
                except:
                    # CUDAæä¾›è€…æœ‰é—®é¢˜ï¼Œå›é€€åˆ°DirectML
                    pass

        # DirectML (Windowsé€šç”¨GPUåŠ é€Ÿ) - æ£€æŸ¥æ˜¯å¦å¯ç”¨
        if (self.system == "Windows" and
            onnx.get('available') and
            'DmlExecutionProvider' in onnx.get('providers', [])):

            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•GPUå¯ç”¨
            has_gpu = (nvidia.get('available') or
                      detection_result.get('amd_gpu', {}).get('available') or
                      detection_result.get('intel_gpu', {}).get('available'))

            return {
                'type': 'directml_gpu',
                'provider': 'DmlExecutionProvider',
                'description': 'DirectML GPUåŠ é€Ÿ (å·²å¯ç”¨)',
                'performance': 'good',
                'gpu_enabled': True,
                'reason': 'DirectMLæ”¯æŒå¤šç§GPUï¼Œå½“å‰å·²é…ç½®å¹¶å¯ç”¨'
            }
        
        # ä»…CPU
        else:
            issues = []
            if not nvidia.get('available'):
                issues.append('æœªæ£€æµ‹åˆ°NVIDIA GPU')
            if not cuda.get('available'):
                issues.append('æœªå®‰è£…CUDA')
            if not onnx.get('available'):
                issues.append('ONNX Runtimeæœªå®‰è£…')
            elif 'CUDAExecutionProvider' not in onnx.get('providers', []):
                issues.append('ONNX Runtimeä¸æ”¯æŒCUDA')
            
            return {
                'type': 'cpu_only',
                'provider': 'CPUExecutionProvider',
                'description': 'CPUå¤„ç†æ¨¡å¼',
                'performance': 'basic',
                'gpu_enabled': False,
                'reason': f"GPUåŠ é€Ÿä¸å¯ç”¨: {', '.join(issues)}"
            }
    
    def _is_gpu_available(self, detection_result: Dict) -> bool:
        """åˆ¤æ–­GPUæ˜¯å¦å¯ç”¨"""
        recommendation = detection_result.get('recommended_config', {})
        return recommendation.get('gpu_enabled', False)
    
    def print_detailed_report(self, detection_result: Dict):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ” GPUç¯å¢ƒæ£€æµ‹æŠ¥å‘Š")
        print("=" * 80)
        
        # ç³»ç»Ÿä¿¡æ¯
        print(f"ğŸ’» æ“ä½œç³»ç»Ÿ: {detection_result['system']}")
        
        # NVIDIA GPU
        nvidia = detection_result['nvidia_gpu']
        print(f"\nğŸ® NVIDIA GPU:")
        if nvidia.get('available'):
            print(f"   âœ… çŠ¶æ€: å¯ç”¨ ({nvidia['count']}ä¸ªGPU)")
            for i, gpu in enumerate(nvidia['gpus']):
                print(f"   ğŸ“Š GPU {i+1}: {gpu['name']}")
                print(f"       ğŸ’¾ æ˜¾å­˜: {gpu['memory_mb']}MB")
                print(f"       ğŸ”§ é©±åŠ¨ç‰ˆæœ¬: {gpu['driver_version']}")
                print(f"       ğŸš€ CUDAç‰ˆæœ¬: {gpu['cuda_version']}")
        else:
            print(f"   âŒ çŠ¶æ€: ä¸å¯ç”¨ ({nvidia.get('error', 'æœªçŸ¥é”™è¯¯')})")
        
        # CUDA
        cuda = detection_result['cuda']
        print(f"\nğŸš€ CUDA:")
        if cuda.get('available'):
            print(f"   âœ… çŠ¶æ€: å¯ç”¨")
            print(f"   ğŸ“‹ ç‰ˆæœ¬: {cuda['version_info']}")
        else:
            print(f"   âŒ çŠ¶æ€: ä¸å¯ç”¨ ({cuda.get('error', 'æœªçŸ¥é”™è¯¯')})")
        
        # ONNX Runtime
        onnx = detection_result['onnx_providers']
        print(f"\nğŸ§  ONNX Runtime:")
        if onnx.get('available'):
            print(f"   âœ… çŠ¶æ€: å¯ç”¨ (ç‰ˆæœ¬ {onnx['onnxruntime_version']})")
            print(f"   ğŸ“‹ å¯ç”¨æä¾›è€…:")
            for provider in onnx['providers']:
                details = onnx['details'].get(provider, {})
                status = "âœ…" if details.get('available', True) else "âŒ"
                desc = details.get('description', provider)
                print(f"       {status} {provider}: {desc}")
        else:
            print(f"   âŒ çŠ¶æ€: ä¸å¯ç”¨ ({onnx.get('error', 'æœªçŸ¥é”™è¯¯')})")
        
        # æ¨èé…ç½®
        recommendation = detection_result['recommended_config']
        print(f"\nğŸ¯ æ¨èé…ç½®:")
        print(f"   ğŸ“‹ ç±»å‹: {recommendation['description']}")
        print(f"   ğŸš€ æä¾›è€…: {recommendation['provider']}")
        print(f"   ğŸ“Š æ€§èƒ½ç­‰çº§: {recommendation['performance']}")
        print(f"   ğŸ’¡ åŸå› : {recommendation['reason']}")
        
        # GPUåŠ é€ŸçŠ¶æ€
        gpu_status = "å¯ç”¨" if detection_result['gpu_available'] else "ç¦ç”¨"
        print(f"\nâš¡ GPUåŠ é€Ÿ: {gpu_status}")
        
        print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    detector = GPUDetector()
    result = detector.detect_all()
    detector.print_detailed_report(result)
    
    return result

if __name__ == "__main__":
    main()
