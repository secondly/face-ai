#!/usr/bin/env python3
"""
ç³»ç»Ÿé…ç½®æ£€æµ‹å™¨
æ£€æµ‹æ‰€æœ‰å¿…è¦çš„é…ç½®é¡¹å¹¶æä¾›è§£å†³æ–¹æ¡ˆ
"""

import os
import sys
import platform
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SystemChecker:
    """ç³»ç»Ÿé…ç½®æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.system = platform.system()
        self.python_version = sys.version_info
        self.project_root = Path(__file__).parent.parent
        
    def check_all(self) -> Dict:
        """æ£€æµ‹æ‰€æœ‰é…ç½®é¡¹"""
        result = {
            'system_info': self._check_system_info(),
            'python_env': self._check_python_environment(),
            'dependencies': self._check_dependencies(),
            'gpu_config': self._check_gpu_configuration(),
            'models': self._check_models(),
            'ffmpeg': self._check_ffmpeg(),
            'onnx_runtime': self._check_onnx_runtime_detailed(),
            'cuda_test': self._test_cuda_functionality(),
            'gpu_performance_test': self._test_gpu_performance(),
            'overall_status': 'unknown',
            'issues': [],
            'recommendations': []
        }
        
        # åˆ†ææ•´ä½“çŠ¶æ€
        result['overall_status'], result['issues'], result['recommendations'] = self._analyze_results(result)
        
        return result
    
    def _check_system_info(self) -> Dict:
        """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
        return {
            'os': self.system,
            'os_version': platform.platform(),
            'python_version': f"{self.python_version.major}.{self.python_version.minor}.{self.python_version.micro}",
            'architecture': platform.architecture()[0],
            'processor': platform.processor(),
            'status': 'ok'
        }
    
    def _check_python_environment(self) -> Dict:
        """æ£€æŸ¥Pythonç¯å¢ƒ"""
        issues = []
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        if self.python_version < (3, 8):
            issues.append("Pythonç‰ˆæœ¬è¿‡ä½ï¼Œå»ºè®®3.8+")
        
        # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
        in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)
        
        # æ£€æŸ¥pip
        pip_available = True
        try:
            import pip
        except ImportError:
            pip_available = False
            issues.append("pipä¸å¯ç”¨")
        
        return {
            'version': f"{self.python_version.major}.{self.python_version.minor}.{self.python_version.micro}",
            'in_virtual_env': in_venv,
            'pip_available': pip_available,
            'executable': sys.executable,
            'issues': issues,
            'status': 'ok' if not issues else 'warning'
        }
    
    def _check_dependencies(self) -> Dict:
        """æ£€æŸ¥Pythonä¾èµ–åŒ…"""
        required_packages = {
            'cv2': 'opencv-python',
            'numpy': 'numpy',
            'insightface': 'insightface',
            'onnxruntime': 'onnxruntime',
            'PyQt5': 'PyQt5',
            'PIL': 'Pillow',
            'tqdm': 'tqdm',
            'rich': 'rich',
            'yaml': 'pyyaml',
            'requests': 'requests'
        }
        
        installed = {}
        missing = []
        
        for import_name, package_name in required_packages.items():
            try:
                if import_name == 'cv2':
                    import cv2
                    installed[package_name] = cv2.__version__
                elif import_name == 'numpy':
                    import numpy
                    installed[package_name] = numpy.__version__
                elif import_name == 'insightface':
                    import insightface
                    installed[package_name] = insightface.__version__
                elif import_name == 'onnxruntime':
                    import onnxruntime
                    installed[package_name] = onnxruntime.__version__
                elif import_name == 'PyQt5':
                    from PyQt5.QtCore import QT_VERSION_STR
                    installed[package_name] = QT_VERSION_STR
                elif import_name == 'PIL':
                    from PIL import Image
                    installed[package_name] = Image.__version__ if hasattr(Image, '__version__') else 'unknown'
                else:
                    module = __import__(import_name)
                    version = getattr(module, '__version__', 'unknown')
                    installed[package_name] = version
            except ImportError:
                missing.append(package_name)
        
        return {
            'installed': installed,
            'missing': missing,
            'status': 'ok' if not missing else 'error'
        }
    
    def _check_gpu_configuration(self) -> Dict:
        """æ£€æŸ¥GPUé…ç½®"""
        try:
            from utils.gpu_detector import GPUDetector
            detector = GPUDetector()
            gpu_result = detector.detect_all()
            
            # æ·»åŠ é¢å¤–çš„GPUé…ç½®æ£€æŸ¥
            gpu_result['cuda_runtime_test'] = self._test_cuda_runtime()
            gpu_result['onnx_gpu_test'] = self._test_onnx_gpu()
            
            return gpu_result
        except Exception as e:
            return {
                'error': str(e),
                'gpu_available': False,
                'status': 'error'
            }
    
    def _test_cuda_runtime(self) -> Dict:
        """æµ‹è¯•CUDAè¿è¡Œæ—¶"""
        try:
            import onnxruntime as ort
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•ä¼šè¯
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            
            # åˆ›å»ºä¸€ä¸ªæœ€å°çš„ONNXæ¨¡å‹è¿›è¡Œæµ‹è¯•
            import numpy as np
            
            # ç®€å•çš„æµ‹è¯•ï¼šæ£€æŸ¥CUDAæä¾›è€…æ˜¯å¦çœŸæ­£å¯ç”¨
            available_providers = ort.get_available_providers()
            if 'CUDAExecutionProvider' in available_providers:
                try:
                    # å°è¯•åˆ›å»ºCUDAä¼šè¯
                    session_options = ort.SessionOptions()
                    session_options.log_severity_level = 3  # å‡å°‘æ—¥å¿—è¾“å‡º
                    
                    # è¿™é‡Œæˆ‘ä»¬åªæ˜¯æµ‹è¯•æä¾›è€…æ˜¯å¦å¯ä»¥åˆå§‹åŒ–
                    return {
                        'cuda_available': True,
                        'providers_available': available_providers,
                        'test_passed': True,
                        'status': 'ok'
                    }
                except Exception as e:
                    return {
                        'cuda_available': False,
                        'error': str(e),
                        'test_passed': False,
                        'status': 'error'
                    }
            else:
                return {
                    'cuda_available': False,
                    'reason': 'CUDAExecutionProvider not in available providers',
                    'test_passed': False,
                    'status': 'warning'
                }
                
        except ImportError as e:
            return {
                'error': f'ONNX Runtime not available: {e}',
                'test_passed': False,
                'status': 'error'
            }
    
    def _test_onnx_gpu(self) -> Dict:
        """æµ‹è¯•ONNX GPUåŠŸèƒ½"""
        try:
            import onnxruntime as ort
            
            # è·å–å¯ç”¨çš„æ‰§è¡Œæä¾›è€…
            providers = ort.get_available_providers()
            
            gpu_providers = []
            if 'CUDAExecutionProvider' in providers:
                gpu_providers.append('CUDAExecutionProvider')
            if 'DmlExecutionProvider' in providers:
                gpu_providers.append('DmlExecutionProvider')
            if 'TensorrtExecutionProvider' in providers:
                gpu_providers.append('TensorrtExecutionProvider')
            
            return {
                'available_providers': providers,
                'gpu_providers': gpu_providers,
                'gpu_available': len(gpu_providers) > 0,
                'recommended_provider': gpu_providers[0] if gpu_providers else 'CPUExecutionProvider',
                'status': 'ok' if gpu_providers else 'warning'
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'status': 'error'
            }

    def _check_models(self) -> Dict:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
        models_dir = self.project_root / "models"
        required_models = {
            'inswapper_128.onnx': 'inswapper_128.onnx',
            'scrfd_10g_bnkps.onnx': 'scrfd_10g_bnkps.onnx',
            'arcface_r100.onnx': 'arcface_r100.onnx'
        }

        existing_models = {}
        missing_models = []

        if models_dir.exists():
            for model_name, file_name in required_models.items():
                model_path = models_dir / file_name
                if model_path.exists():
                    existing_models[model_name] = {
                        'path': str(model_path),
                        'size': model_path.stat().st_size,
                        'exists': True
                    }
                else:
                    missing_models.append(model_name)
        else:
            missing_models = list(required_models.keys())

        return {
            'models_dir': str(models_dir),
            'existing': existing_models,
            'missing': missing_models,
            'status': 'ok' if not missing_models else 'warning'
        }

    def _check_ffmpeg(self) -> Dict:
        """æ£€æŸ¥FFmpeg"""
        try:
            # æ£€æŸ¥ç³»ç»ŸPATHä¸­çš„ffmpeg
            result = subprocess.run(['ffmpeg', '-version'],
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                version_line = result.stdout.split('\n')[0]
                return {
                    'available': True,
                    'version': version_line,
                    'location': 'system',
                    'status': 'ok'
                }
        except (FileNotFoundError, subprocess.TimeoutExpired):
            pass

        # æ£€æŸ¥é¡¹ç›®ç›®å½•ä¸­çš„ffmpeg
        ffmpeg_dir = self.project_root / "ffmpeg"
        if ffmpeg_dir.exists():
            ffmpeg_exe = ffmpeg_dir / "ffmpeg.exe" if self.system == "Windows" else ffmpeg_dir / "ffmpeg"
            if ffmpeg_exe.exists():
                try:
                    result = subprocess.run([str(ffmpeg_exe), '-version'],
                                          capture_output=True, text=True, timeout=10)
                    if result.returncode == 0:
                        version_line = result.stdout.split('\n')[0]
                        return {
                            'available': True,
                            'version': version_line,
                            'location': 'local',
                            'path': str(ffmpeg_exe),
                            'status': 'ok'
                        }
                except Exception:
                    pass

        return {
            'available': False,
            'status': 'error'
        }

    def _check_onnx_runtime_detailed(self) -> Dict:
        """è¯¦ç»†æ£€æŸ¥ONNX Runtime"""
        try:
            import onnxruntime as ort

            # åŸºæœ¬ä¿¡æ¯
            version = ort.__version__
            providers = ort.get_available_providers()

            # æ£€æŸ¥GPUæä¾›è€…çš„è¯¦ç»†çŠ¶æ€
            provider_details = {}

            for provider in providers:
                if provider == 'CUDAExecutionProvider':
                    try:
                        # å°è¯•è·å–CUDAè®¾å¤‡ä¿¡æ¯
                        device_info = ort.get_device()
                        provider_details[provider] = {
                            'available': True,
                            'device_info': str(device_info),
                            'status': 'ok'
                        }
                    except Exception as e:
                        provider_details[provider] = {
                            'available': False,
                            'error': str(e),
                            'status': 'error'
                        }
                else:
                    provider_details[provider] = {
                        'available': True,
                        'status': 'ok'
                    }

            return {
                'version': version,
                'providers': providers,
                'provider_details': provider_details,
                'gpu_providers': [p for p in providers if 'GPU' in p or 'CUDA' in p or 'Dml' in p or 'Tensorrt' in p],
                'status': 'ok'
            }

        except ImportError:
            return {
                'available': False,
                'error': 'ONNX Runtime not installed',
                'status': 'error'
            }
        except Exception as e:
            return {
                'error': str(e),
                'status': 'error'
            }

    def _test_cuda_functionality(self) -> Dict:
        """æµ‹è¯•CUDAåŠŸèƒ½æ˜¯å¦çœŸæ­£å¯ç”¨"""
        try:
            import onnxruntime as ort
            import numpy as np

            # æ£€æŸ¥CUDAæä¾›è€…æ˜¯å¦å¯ç”¨
            providers = ort.get_available_providers()
            if 'CUDAExecutionProvider' not in providers:
                return {
                    'test_passed': False,
                    'reason': 'CUDAExecutionProvider not available',
                    'status': 'warning'
                }

            # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„CUDAä¼šè¯è¿›è¡Œå®é™…æµ‹è¯•
            try:
                # åˆ›å»ºä¼šè¯é€‰é¡¹
                session_options = ort.SessionOptions()
                session_options.log_severity_level = 3  # å‡å°‘æ—¥å¿—è¾“å‡º

                # è¿™é‡Œæˆ‘ä»¬åªæ˜¯éªŒè¯CUDAæä¾›è€…å¯ä»¥è¢«åˆå§‹åŒ–
                # å®é™…çš„æ¨¡å‹æµ‹è¯•ä¼šåœ¨æ¨¡å‹åŠ è½½æ—¶è¿›è¡Œ
                return {
                    'test_passed': True,
                    'cuda_provider_available': True,
                    'status': 'ok'
                }

            except Exception as e:
                return {
                    'test_passed': False,
                    'cuda_provider_available': False,
                    'error': str(e),
                    'status': 'error'
                }

        except ImportError:
            return {
                'test_passed': False,
                'error': 'Required modules not available',
                'status': 'error'
            }
        except Exception as e:
            return {
                'test_passed': False,
                'error': str(e),
                'status': 'error'
            }

    def _analyze_results(self, result: Dict) -> Tuple[str, List[str], List[str]]:
        """åˆ†ææ£€æµ‹ç»“æœå¹¶ç”Ÿæˆé—®é¢˜å’Œå»ºè®®"""
        issues = []
        recommendations = []

        # æ£€æŸ¥Pythonç¯å¢ƒ
        python_env = result['python_env']
        if python_env['status'] != 'ok':
            issues.extend(python_env['issues'])
            if not python_env['in_virtual_env']:
                recommendations.append("å»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒæ¥ç®¡ç†PythonåŒ…")

        # æ£€æŸ¥ä¾èµ–åŒ…
        dependencies = result['dependencies']
        if dependencies['missing']:
            issues.append(f"ç¼ºå°‘å¿…è¦çš„PythonåŒ…: {', '.join(dependencies['missing'])}")
            recommendations.append("è¿è¡Œå‘½ä»¤å®‰è£…ä¾èµ–: pip install -r requirements.txt")

        # æ£€æŸ¥GPUé…ç½®
        gpu_config = result['gpu_config']
        if not gpu_config.get('gpu_available', False):
            issues.append("GPUåŠ é€Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
            if gpu_config.get('nvidia_gpu', {}).get('available'):
                if not gpu_config.get('cuda', {}).get('available'):
                    recommendations.append("å®‰è£…CUDAå·¥å…·åŒ…ä»¥å¯ç”¨GPUåŠ é€Ÿ")
                elif 'CUDAExecutionProvider' not in gpu_config.get('onnx_providers', {}).get('providers', []):
                    recommendations.append("å®‰è£…onnxruntime-gpuä»¥å¯ç”¨GPUåŠ é€Ÿ: pip install onnxruntime-gpu")

        # æ£€æŸ¥CUDAæµ‹è¯•
        cuda_test = result['cuda_test']
        if not cuda_test.get('test_passed', False):
            if cuda_test.get('status') == 'error':
                issues.append(f"CUDAåŠŸèƒ½æµ‹è¯•å¤±è´¥: {cuda_test.get('error', 'æœªçŸ¥é”™è¯¯')}")
                recommendations.append("æ£€æŸ¥CUDAå®‰è£…å’ŒGPUé©±åŠ¨ç¨‹åº")

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        models = result['models']
        if models['missing']:
            issues.append(f"ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {', '.join(models['missing'])}")
            recommendations.append("è¿è¡Œæ¨¡å‹ä¸‹è½½å™¨: python scripts/download_models.py")

        # æ£€æŸ¥FFmpeg
        ffmpeg = result['ffmpeg']
        if not ffmpeg['available']:
            issues.append("FFmpegä¸å¯ç”¨ï¼Œè§†é¢‘å¤„ç†åŠŸèƒ½å°†å—é™")
            recommendations.append("å®‰è£…FFmpegæˆ–è¿è¡Œ: python download_ffmpeg.py")

        # æ£€æŸ¥ONNX Runtimeè¯¦ç»†çŠ¶æ€
        onnx_runtime = result['onnx_runtime']
        if onnx_runtime.get('status') == 'error':
            issues.append("ONNX Runtimeé…ç½®æœ‰é—®é¢˜")
            recommendations.append("é‡æ–°å®‰è£…ONNX Runtime: pip install --upgrade onnxruntime")

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if not issues:
            overall_status = 'excellent'
        elif any('GPU' in issue or 'CUDA' in issue for issue in issues):
            overall_status = 'good'  # GPUé—®é¢˜ä¸å½±å“åŸºæœ¬åŠŸèƒ½
        elif any('ç¼ºå°‘' in issue for issue in issues):
            overall_status = 'warning'  # ç¼ºå°‘ç»„ä»¶
        else:
            overall_status = 'error'  # ä¸¥é‡é—®é¢˜

        return overall_status, issues, recommendations

    def print_summary(self, result: Dict):
        """æ‰“å°æ£€æµ‹ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ” ç³»ç»Ÿé…ç½®æ£€æµ‹æŠ¥å‘Š")
        print("=" * 80)

        # ç³»ç»Ÿä¿¡æ¯
        system_info = result['system_info']
        print(f"ğŸ’» ç³»ç»Ÿ: {system_info['os']} {system_info['architecture']}")
        print(f"ğŸ Python: {system_info['python_version']}")

        # æ•´ä½“çŠ¶æ€
        status_icons = {
            'excellent': 'ğŸŸ¢',
            'good': 'ğŸŸ¡',
            'warning': 'ğŸŸ ',
            'error': 'ğŸ”´'
        }
        status_icon = status_icons.get(result['overall_status'], 'â“')
        print(f"\n{status_icon} æ•´ä½“çŠ¶æ€: {result['overall_status'].upper()}")

        # é—®é¢˜åˆ—è¡¨
        if result['issues']:
            print(f"\nâŒ å‘ç°é—®é¢˜ ({len(result['issues'])}ä¸ª):")
            for i, issue in enumerate(result['issues'], 1):
                print(f"   {i}. {issue}")

        # å»ºè®®åˆ—è¡¨
        if result['recommendations']:
            print(f"\nğŸ’¡ å»ºè®®æ“ä½œ ({len(result['recommendations'])}ä¸ª):")
            for i, rec in enumerate(result['recommendations'], 1):
                print(f"   {i}. {rec}")

        # GPUçŠ¶æ€
        gpu_config = result['gpu_config']
        if gpu_config.get('gpu_available'):
            recommended = gpu_config.get('recommended_config', {})
            print(f"\nğŸš€ GPUåŠ é€Ÿ: å¯ç”¨ ({recommended.get('description', 'Unknown')})")
        else:
            print(f"\nğŸ’» GPUåŠ é€Ÿ: ä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")

        print("=" * 80)

    def _test_gpu_performance(self) -> Dict:
        """æµ‹è¯•GPUæ€§èƒ½"""
        try:
            from utils.gpu_tester import GPUTester

            tester = GPUTester()

            # æµ‹è¯•ONNX GPUåŠŸèƒ½
            onnx_result = tester.test_onnx_gpu_functionality()

            # æµ‹è¯•InsightFace GPUåŠŸèƒ½
            insightface_result = tester.test_insightface_gpu()

            return {
                'onnx_test': onnx_result,
                'insightface_test': insightface_result,
                'overall_gpu_usable': self._determine_gpu_usability(onnx_result, insightface_result),
                'status': 'ok' if onnx_result.get('status') in ['excellent', 'good'] else 'warning'
            }

        except Exception as e:
            return {
                'error': str(e),
                'status': 'error'
            }

    def _determine_gpu_usability(self, onnx_result: Dict, insightface_result: Dict) -> Dict:
        """ç¡®å®šGPUæ˜¯å¦çœŸæ­£å¯ç”¨"""
        # æ£€æŸ¥ONNX GPUæµ‹è¯•ç»“æœ
        onnx_gpu_works = False
        if onnx_result.get('status') in ['excellent', 'good']:
            if onnx_result.get('cuda_test', {}).get('success') or onnx_result.get('directml_test', {}).get('success'):
                onnx_gpu_works = True

        # æ£€æŸ¥InsightFace GPUæµ‹è¯•ç»“æœ
        insightface_gpu_works = insightface_result.get('gpu_available', False)

        # ç»¼åˆåˆ¤æ–­
        if onnx_gpu_works and insightface_gpu_works:
            return {
                'gpu_fully_functional': True,
                'recommendation': 'GPUåŠ é€Ÿå®Œå…¨å¯ç”¨ï¼Œå»ºè®®ä½¿ç”¨GPUæ¨¡å¼',
                'performance_level': 'excellent'
            }
        elif onnx_gpu_works:
            return {
                'gpu_fully_functional': False,
                'recommendation': 'ONNX GPUå¯ç”¨ä½†InsightFace GPUæœ‰é—®é¢˜ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é…ç½®',
                'performance_level': 'good',
                'issue': 'InsightFace GPUåˆå§‹åŒ–å¤±è´¥'
            }
        else:
            return {
                'gpu_fully_functional': False,
                'recommendation': 'GPUåŠ é€Ÿä¸å¯ç”¨ï¼Œå»ºè®®ä½¿ç”¨CPUæ¨¡å¼æˆ–æ£€æŸ¥GPUé…ç½®',
                'performance_level': 'basic',
                'issue': 'GPUåŠŸèƒ½æµ‹è¯•å¤±è´¥'
            }
