#!/usr/bin/env python3
"""
GPUç¯å¢ƒæµ‹è¯•è„šæœ¬
ç”¨äºè¯Šæ–­CUDAè™šæ‹Ÿç¯å¢ƒä¸­çš„GPUé…ç½®é—®é¢˜
"""

import os
import sys
import subprocess

def check_environment():
    """æ£€æŸ¥å½“å‰ç¯å¢ƒ"""
    print("ğŸ” ç¯å¢ƒæ£€æŸ¥")
    print("=" * 50)
    
    # æ£€æŸ¥condaç¯å¢ƒ
    conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'æœªçŸ¥')
    print(f"Condaç¯å¢ƒ: {conda_env}")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥Pythonè·¯å¾„
    print(f"Pythonè·¯å¾„: {sys.executable}")
    print()

def check_cuda():
    """æ£€æŸ¥CUDA"""
    print("ğŸš€ CUDAæ£€æŸ¥")
    print("=" * 50)
    
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… CUDAå¯ç”¨")
            print(result.stdout)
        else:
            print("âŒ CUDAä¸å¯ç”¨")
            print(result.stderr)
    except Exception as e:
        print(f"âŒ CUDAæ£€æŸ¥å¤±è´¥: {e}")
    print()

def check_onnxruntime():
    """æ£€æŸ¥ONNX Runtime"""
    print("ğŸ§  ONNX Runtimeæ£€æŸ¥")
    print("=" * 50)
    
    try:
        import onnxruntime as ort
        print(f"âœ… ONNX Runtimeç‰ˆæœ¬: {ort.__version__}")
        
        providers = ort.get_available_providers()
        print(f"å¯ç”¨æä¾›è€…: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("âœ… CUDAæä¾›è€…å¯ç”¨")
            
            # å°è¯•åˆ›å»ºCUDAä¼šè¯
            try:
                session = ort.InferenceSession(
                    None,  # ç©ºæ¨¡å‹
                    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
                )
                print("âœ… CUDAä¼šè¯åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                print(f"âŒ CUDAä¼šè¯åˆ›å»ºå¤±è´¥: {e}")
        else:
            print("âŒ CUDAæä¾›è€…ä¸å¯ç”¨")
            
    except ImportError:
        print("âŒ ONNX Runtimeæœªå®‰è£…")
    except Exception as e:
        print(f"âŒ ONNX Runtimeæ£€æŸ¥å¤±è´¥: {e}")
    print()

def check_pytorch():
    """æ£€æŸ¥PyTorchï¼ˆå¯é€‰ï¼‰"""
    print("ğŸ”¥ PyTorchæ£€æŸ¥")
    print("=" * 50)
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
            print(f"å½“å‰CUDAè®¾å¤‡: {torch.cuda.current_device()}")
            print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name()}")
    except ImportError:
        print("âš ï¸ PyTorchæœªå®‰è£…")
    except Exception as e:
        print(f"âŒ PyTorchæ£€æŸ¥å¤±è´¥: {e}")
    print()

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜"""
    print("ğŸ’¾ GPUå†…å­˜æ£€æŸ¥")
    print("=" * 50)
    
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… nvidia-smiå¯ç”¨")
            # åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
            lines = result.stdout.split('\n')
            for line in lines:
                if 'MiB' in line or 'GPU' in line or 'Driver Version' in line:
                    print(line.strip())
        else:
            print("âŒ nvidia-smiä¸å¯ç”¨")
    except Exception as e:
        print(f"âŒ GPUå†…å­˜æ£€æŸ¥å¤±è´¥: {e}")
    print()

def main():
    print("ğŸ¯ GPUç¯å¢ƒè¯Šæ–­å·¥å…·")
    print("=" * 80)
    print()
    
    check_environment()
    check_cuda()
    check_onnxruntime()
    check_pytorch()
    check_gpu_memory()
    
    print("ğŸ¯ è¯Šæ–­å®Œæˆ")
    print("=" * 80)
    print()
    print("ğŸ’¡ å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š")
    print("1. æ˜¯å¦åœ¨æ­£ç¡®çš„condaç¯å¢ƒä¸­ (face-ai-cuda11)")
    print("2. ONNX Runtimeç‰ˆæœ¬æ˜¯å¦å…¼å®¹")
    print("3. CUDAå·¥å…·åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…")
    print("4. æ˜¾å¡é©±åŠ¨æ˜¯å¦æœ€æ–°")

if __name__ == "__main__":
    main()
